{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2a54d23-4b0e-44c5-b67b-6b31fd8f3ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting summa\n",
      "  Downloading summa-1.2.0.tar.gz (54 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.19 in /opt/conda/lib/python3.12/site-packages (from summa) (1.15.2)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in /opt/conda/lib/python3.12/site-packages (from scipy>=0.19->summa) (2.2.6)\n",
      "Building wheels for collected packages: summa\n",
      "\u001b[33m  DEPRECATION: Building 'summa' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'summa'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for summa (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for summa: filename=summa-1.2.0-py3-none-any.whl size=54441 sha256=4c346584f2eb834cafb630723d7cb5c0c256541dfa026243ccf24465af975a2b\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/70/26/84/58df5a55ebde6fd802666b6ac0b86909ecd018a2702b89d13c\n",
      "Successfully built summa\n",
      "Installing collected packages: summa\n",
      "Successfully installed summa-1.2.0\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.12/site-packages (from nltk) (1.5.1)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2025.7.34-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.7.34-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (801 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.9/801.9 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, nltk\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [nltk][32m1/2\u001b[0m [nltk]\n",
      "\u001b[1A\u001b[2KSuccessfully installed nltk-3.9.1 regex-2025.7.34\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade summa\n",
    "!pip install --upgrade nltk #3.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53857b3d-ca7f-4a43-98cb-79b20f96021e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.1\n",
      "2.7.1+cu118\n",
      "2.3.0\n",
      "1.2.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import nltk\n",
    "import torch\n",
    "import summa\n",
    "import pandas as pd\n",
    "\n",
    "print(nltk.__version__)\n",
    "print(torch.__version__)\n",
    "print(pd.__version__)\n",
    "print(version('summa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "993e9704-5cc0-4a85-909a-6263d1f331d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/sunnysai12345/News_Summary/master/news_summary_more.csv\", filename=\"news_summary_more.csv\")\n",
    "data = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c54e4580-886c-49dc-8daf-1e97f47f0e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85112</th>\n",
       "      <td>Hong Kong parking space sells for over Ã¢ÂÂ¹4...</td>\n",
       "      <td>A 188-square-foot parking space in Hong Kong h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>Over 1.5 crore people take 'Shahi Snan' on day...</td>\n",
       "      <td>To mark the beginning of the Kumbh Mela in Pra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48325</th>\n",
       "      <td>Anti-trafficking law proposes life term for re...</td>\n",
       "      <td>India's first law against human trafficking, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7872</th>\n",
       "      <td>Kerala plans 600-km wall with women to prevent...</td>\n",
       "      <td>Kerala minister Thomas Isaac on Sunday announc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15088</th>\n",
       "      <td>I know you never think: US Prez Trump mocks re...</td>\n",
       "      <td>US President Donald Trump on Monday mocked a r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43581</th>\n",
       "      <td>May support no-confidence motion against NDA g...</td>\n",
       "      <td>Andhra Pradesh CM and TDP chief Chandrababu Na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18763</th>\n",
       "      <td>When doc asks you to lose weight: Smriti Irani...</td>\n",
       "      <td>Actress-turned-politician Smriti Irani on Tues...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31601</th>\n",
       "      <td>Suffered from depression, had suicidal thought...</td>\n",
       "      <td>Zaira Wasim, known for starring in the film 'D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86563</th>\n",
       "      <td>Upset 7-Eleven outlet owner opens 6-Twelve acr...</td>\n",
       "      <td>The former owner of a 7-Eleven Boston outlet h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63933</th>\n",
       "      <td>Kiran Bedi jumps over hospital fence as offici...</td>\n",
       "      <td>Puducherry Lieutenant Governor Kiran Bedi jump...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               headlines  \\\n",
       "85112  Hong Kong parking space sells for over Ã¢ÂÂ¹4...   \n",
       "2064   Over 1.5 crore people take 'Shahi Snan' on day...   \n",
       "48325  Anti-trafficking law proposes life term for re...   \n",
       "7872   Kerala plans 600-km wall with women to prevent...   \n",
       "15088  I know you never think: US Prez Trump mocks re...   \n",
       "43581  May support no-confidence motion against NDA g...   \n",
       "18763  When doc asks you to lose weight: Smriti Irani...   \n",
       "31601  Suffered from depression, had suicidal thought...   \n",
       "86563  Upset 7-Eleven outlet owner opens 6-Twelve acr...   \n",
       "63933  Kiran Bedi jumps over hospital fence as offici...   \n",
       "\n",
       "                                                    text  \n",
       "85112  A 188-square-foot parking space in Hong Kong h...  \n",
       "2064   To mark the beginning of the Kumbh Mela in Pra...  \n",
       "48325  India's first law against human trafficking, l...  \n",
       "7872   Kerala minister Thomas Isaac on Sunday announc...  \n",
       "15088  US President Donald Trump on Monday mocked a r...  \n",
       "43581  Andhra Pradesh CM and TDP chief Chandrababu Na...  \n",
       "18763  Actress-turned-politician Smriti Irani on Tues...  \n",
       "31601  Zaira Wasim, known for starring in the film 'D...  \n",
       "86563  The former owner of a 7-Eleven Boston outlet h...  \n",
       "63933  Puducherry Lieutenant Governor Kiran Bedi jump...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "195a7dd2-5041-4f17-892e-608314dcfa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2. 추상적 요약을 위해 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14b638f5-9afc-40aa-83e1-7588b8e144f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.12/site-packages (6.0.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement parser (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for parser\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b422917d-540f-4dce-bd6c-e337cd82cf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 열에서 중복을 배제한 유일한 샘플의 수 : 98360\n",
      "전체 샘플수 (중복 제거 후) : 98360\n",
      "전체 샘플수 (NULL 제거 후) : 98360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트의 최소 길이 : 1\n",
      "텍스트의 최대 길이 : 60\n",
      "텍스트의 평균 길이 : 35.09968483123221\n",
      "요약의 최소 길이 : 1\n",
      "요약의 최대 길이 : 16\n",
      "요약의 평균 길이 : 9.299532330215534\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 (길이 필터링 후) : 27105\n",
      "훈련 데이터의 개수 : 21684\n",
      "테스트 데이터의 개수 : 5421\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "# --- 1. 데이터 로드 및 기본 전처리 ---\n",
    "# 데이터를 불러오고 'Text'와 'Summary' 열만 선택합니다.\n",
    "data = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1')\n",
    "data = data[['text', 'headlines']]\n",
    "data.columns = ['Text', 'Summary'] # 열 이름 변경\n",
    "\n",
    "# 중복 샘플과 NULL 값 제거\n",
    "print('Text 열에서 중복을 배제한 유일한 샘플의 수 :', data['Text'].nunique())\n",
    "data.drop_duplicates(subset=['Text'], inplace=True)\n",
    "print('전체 샘플수 (중복 제거 후) :', (len(data)))\n",
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 (NULL 제거 후) :', (len(data)))\n",
    "\n",
    "\n",
    "# --- 2. 텍스트 정규화 및 정제 ---\n",
    "# 정규화 사전 (축약어 처리)\n",
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "# NLTK 불용어 로드\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 전처리 함수\n",
    "def preprocess_sentence(sentence, remove_stopwords=True):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = BeautifulSoup(sentence, \"lxml\").text\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence)\n",
    "    sentence = re.sub('\"','', sentence)\n",
    "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")])\n",
    "    sentence = re.sub(r\"'s\\b\",\"\", sentence)\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
    "    sentence = re.sub('[m]{2,}', 'mm', sentence)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        tokens = ' '.join(word for word in sentence.split() if not word in stop_words if len(word) > 1)\n",
    "    else:\n",
    "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "    return tokens\n",
    "\n",
    "# Text 열은 불용어 제거, Summary 열은 불용어 미제거\n",
    "data['Text'] = data['Text'].apply(lambda x: preprocess_sentence(str(x)))\n",
    "data['Summary'] = data['Summary'].apply(lambda x: preprocess_sentence(str(x), False))\n",
    "\n",
    "# 빈 값을 Null로 변환 후 제거\n",
    "data.replace('', np.nan, inplace=True)\n",
    "data.dropna(axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# --- 3. 샘플 길이 분석 및 필터링 ---\n",
    "text_len = [len(s.split()) for s in data['Text']]\n",
    "summary_len = [len(s.split()) for s in data['Summary']]\n",
    "\n",
    "print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))\n",
    "print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\n",
    "print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))\n",
    "print('요약의 최소 길이 : {}'.format(np.min(summary_len)))\n",
    "print('요약의 최대 길이 : {}'.format(np.max(summary_len)))\n",
    "print('요약의 평균 길이 : {}'.format(np.mean(summary_len)))\n",
    "\n",
    "# 길이 분포 시각화\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(text_len)\n",
    "plt.title('Text')\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(summary_len)\n",
    "plt.title('Summary')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 최대 길이 설정 (분석 결과에 따라 조정 가능)\n",
    "text_max_len = 50\n",
    "summary_max_len = 8\n",
    "\n",
    "# 최대 길이보다 긴 샘플 필터링\n",
    "data = data[data['Text'].apply(lambda x: len(x.split()) <= text_max_len)]\n",
    "data = data[data['Summary'].apply(lambda x: len(x.split()) <= summary_max_len)]\n",
    "print('전체 샘플수 (길이 필터링 후) :', (len(data)))\n",
    "\n",
    "\n",
    "# --- 4. 디코더 입출력 생성 및 데이터 분할 ---\n",
    "# 시작 토큰(sostoken)과 종료 토큰(eostoken) 추가\n",
    "data['decoder_input'] = data['Summary'].apply(lambda x : 'sostoken '+ x)\n",
    "data['decoder_target'] = data['Summary'].apply(lambda x : x + ' eostoken')\n",
    "\n",
    "# Numpy 배열로 변환\n",
    "encoder_input = np.array(data['Text'])\n",
    "decoder_input = np.array(data['decoder_input'])\n",
    "decoder_target = np.array(data['decoder_target'])\n",
    "\n",
    "# 데이터 셔플\n",
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]\n",
    "\n",
    "# 8:2 비율로 훈련 데이터와 테스트 데이터 분리\n",
    "n_of_val = int(len(encoder_input)*0.2)\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "\n",
    "\n",
    "# --- 5. 정수 인코딩 및 패딩 ---\n",
    "# 소스(Text) 데이터 단어 집합 생성\n",
    "src_vocab_size = 8000\n",
    "def build_limited_vocab(texts, vocab_size):\n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    word_counter = Counter()\n",
    "    for text in texts:\n",
    "        word_counter.update(text.split())\n",
    "    for word, _ in word_counter.most_common(vocab_size - 2):\n",
    "        vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "src_vocab = build_limited_vocab(encoder_input_train, src_vocab_size)\n",
    "\n",
    "# 텍스트를 정수 시퀀스로 변환하는 함수\n",
    "def text_to_sequence(texts, vocab):\n",
    "    sequences = []\n",
    "    for text in texts:\n",
    "        sequence = [vocab.get(word, vocab[\"<UNK>\"]) for word in text.split()]\n",
    "        sequences.append(sequence)\n",
    "    return sequences\n",
    "\n",
    "encoder_input_train_seq = text_to_sequence(encoder_input_train, src_vocab)\n",
    "encoder_input_test_seq = text_to_sequence(encoder_input_test, src_vocab)\n",
    "\n",
    "# 타겟(Summary) 데이터 단어 집합 생성\n",
    "tar_vocab_size = 5000\n",
    "tar_vocab = build_limited_vocab(np.concatenate([decoder_input_train, decoder_target_train]), tar_vocab_size)\n",
    "tar_word_to_index = tar_vocab\n",
    "tar_index_to_word = {idx: word for word, idx in tar_vocab.items()}\n",
    "\n",
    "decoder_input_train_seq = text_to_sequence(decoder_input_train, tar_vocab)\n",
    "decoder_target_train_seq = text_to_sequence(decoder_target_train, tar_vocab)\n",
    "decoder_input_test_seq = text_to_sequence(decoder_input_test, tar_vocab)\n",
    "decoder_target_test_seq = text_to_sequence(decoder_target_test, tar_vocab)\n",
    "\n",
    "# 패딩 처리 함수\n",
    "def pad_sequences_pytorch(sequences, maxlen, padding_value=0):\n",
    "    tensors = [torch.tensor(seq, dtype=torch.long) for seq in sequences]\n",
    "    padded_seqs = pad_sequence(tensors, batch_first=True, padding_value=padding_value)\n",
    "    # maxlen보다 길면 자르고, 짧으면 0으로 채워진 상태\n",
    "    if padded_seqs.shape[1] > maxlen:\n",
    "        return padded_seqs[:, :maxlen]\n",
    "    return padded_seqs\n",
    "\n",
    "encoder_input_train = pad_sequences_pytorch(encoder_input_train_seq, maxlen=text_max_len)\n",
    "encoder_input_test = pad_sequences_pytorch(encoder_input_test_seq, maxlen=text_max_len)\n",
    "decoder_input_train = pad_sequences_pytorch(decoder_input_train_seq, maxlen=summary_max_len)\n",
    "decoder_target_train = pad_sequences_pytorch(decoder_target_train_seq, maxlen=summary_max_len)\n",
    "decoder_input_test = pad_sequences_pytorch(decoder_input_test_seq, maxlen=summary_max_len)\n",
    "decoder_target_test = pad_sequences_pytorch(decoder_target_test_seq, maxlen=summary_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2e2436c-911a-4a34-aa21-fa103bb46fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step3. 어텐션 메커니즘을 사용한 seq2seq 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b6847cc-3e4b-43d8-a81d-9ad8d237445d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 아키텍처 정의 완료.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- 하이퍼파라미터 정의 ---\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "num_layers = 3  # LSTM 레이어의 수\n",
    "dropout_p = 0.4 # 드롭아웃 확률\n",
    "\n",
    "# --- 1.1. 인코더 (Encoder) 정의 ---\n",
    "# 입력 문장을 읽어 하나의 컨텍스트 벡터(Context Vector)로 압축합니다.\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_size, num_layers=num_layers,\n",
    "            dropout=dropout, batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        # output: (batch_size, seq_len, hidden_size)\n",
    "        # hidden, cell: (num_layers, batch_size, hidden_size)\n",
    "        return output, hidden, cell\n",
    "\n",
    "# --- 1.2. 디코더 (Decoder) 정의 ---\n",
    "# 컨텍스트 벡터를 받아 요약 문장을 생성합니다.\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_size, num_layers=num_layers,\n",
    "            dropout=dropout, batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x: (batch_size, 1) - 한 번에 하나의 단어만 입력받음\n",
    "        embedded = self.embedding(x) # (batch_size, 1, embedding_dim)\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        # output: (batch_size, 1, hidden_size)\n",
    "        return output, hidden, cell\n",
    "\n",
    "# --- 1.3. 어텐션 (Attention) 메커니즘 정의 ---\n",
    "# 디코더가 예측할 때 인코더의 모든 출력(단어) 중 어떤 단어에 집중할지 결정합니다.\n",
    "class Attention_dot(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention_dot, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, decoder_output, encoder_outputs):\n",
    "        # decoder_output: (batch_size, 1, hidden_size)\n",
    "        # encoder_outputs: (batch_size, seq_len, hidden_size)\n",
    "        attn_weights = torch.bmm(decoder_output, encoder_outputs.transpose(1, 2))\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1) # (batch_size, 1, seq_len)\n",
    "        attn_out = torch.bmm(attn_weights, encoder_outputs) # (batch_size, 1, hidden_size)\n",
    "        return attn_out\n",
    "\n",
    "# --- 1.4. Seq2Seq 모델 결합 ---\n",
    "# 인코더, 디코더, 어텐션을 하나로 묶어 최종 모델을 완성합니다.\n",
    "class Seq2SeqWithAttention(nn.Module):\n",
    "    def __init__(self, encoder, decoder, vocab_size, hidden_size):\n",
    "        super(Seq2SeqWithAttention, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.attention = Attention_dot(hidden_size)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size) # 어텐션 결과를 결합하기 위한 레이어\n",
    "        self.output_layer = nn.Linear(hidden_size, vocab_size) # 최종 단어 예측을 위한 출력 레이어\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        encoder_outputs, hidden, cell = self.encoder(encoder_input)\n",
    "        \n",
    "        # 학습 시에는 teacher forcing을 위해 전체 요약 문장을 한 번에 처리\n",
    "        decoder_outputs, _, _ = self.decoder(decoder_input, hidden, cell)\n",
    "\n",
    "        # 어텐션 적용\n",
    "        attn_out = self.attention(decoder_outputs, encoder_outputs)\n",
    "        \n",
    "        # 어텐션 결과와 디코더 출력을 연결 후 tanh 활성화 함수 적용\n",
    "        decoder_concat_output = torch.cat((decoder_outputs, attn_out), dim=-1)\n",
    "        decoder_concat_output = torch.tanh(self.concat(decoder_concat_output))\n",
    "        \n",
    "        # 최종 출력\n",
    "        output = self.output_layer(decoder_concat_output)\n",
    "        return output\n",
    "\n",
    "print(\"모델 아키텍처 정의 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7b0e115-da04-43f5-88e8-135e0b0c99b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "학습 환경 설정 완료.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# --- 2.1. 모델 인스턴스화 ---\n",
    "encoder = Encoder(src_vocab_size, embedding_dim, hidden_size, num_layers, dropout_p)\n",
    "decoder = Decoder(tar_vocab_size, embedding_dim, hidden_size, num_layers, dropout_p)\n",
    "model = Seq2SeqWithAttention(encoder, decoder, tar_vocab_size, hidden_size)\n",
    "\n",
    "# --- 2.2. 장치 설정 및 모델 이동 ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "# --- 2.3. 학습 관련 하이퍼파라미터 ---\n",
    "batch_size = 256\n",
    "epochs = 50\n",
    "learning_rate = 0.001\n",
    "patience = 2  # 조기 종료(Early Stopping)를 위한 patience 값\n",
    "\n",
    "# --- 2.4. 손실 함수 및 옵티마이저 ---\n",
    "# 패딩 토큰(인덱스 0)은 손실 계산에서 무시\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# --- 2.5. PyTorch DataLoader 설정 ---\n",
    "# 전처리된 데이터를 TensorDataset으로 묶어줍니다.\n",
    "train_dataset = TensorDataset(encoder_input_train, decoder_input_train, decoder_target_train)\n",
    "test_dataset = TensorDataset(encoder_input_test, decoder_input_test, decoder_target_test)\n",
    "\n",
    "# DataLoader를 통해 배치 단위로 데이터를 공급합니다.\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"학습 환경 설정 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07f5b287-77b2-4293-877c-605c0d2742ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 6.8156 | Val Loss: 6.3780\n",
      "Epoch 2/50 | Train Loss: 6.5088 | Val Loss: 6.3048\n",
      "Epoch 3/50 | Train Loss: 6.4421 | Val Loss: 6.2912\n",
      "Epoch 4/50 | Train Loss: 6.3566 | Val Loss: 6.1499\n",
      "Epoch 5/50 | Train Loss: 6.1980 | Val Loss: 6.0078\n",
      "Epoch 6/50 | Train Loss: 6.0177 | Val Loss: 5.8543\n",
      "Epoch 7/50 | Train Loss: 5.8431 | Val Loss: 5.7306\n",
      "Epoch 8/50 | Train Loss: 5.6676 | Val Loss: 5.6129\n",
      "Epoch 9/50 | Train Loss: 5.4964 | Val Loss: 5.5114\n",
      "Epoch 10/50 | Train Loss: 5.3391 | Val Loss: 5.4404\n",
      "Epoch 11/50 | Train Loss: 5.1815 | Val Loss: 5.3594\n",
      "Epoch 12/50 | Train Loss: 5.0243 | Val Loss: 5.2927\n",
      "Epoch 13/50 | Train Loss: 4.8726 | Val Loss: 5.2408\n",
      "Epoch 14/50 | Train Loss: 4.7285 | Val Loss: 5.1946\n",
      "Epoch 15/50 | Train Loss: 4.5852 | Val Loss: 5.1777\n",
      "Epoch 16/50 | Train Loss: 4.4574 | Val Loss: 5.1560\n",
      "Epoch 17/50 | Train Loss: 4.3265 | Val Loss: 5.1411\n",
      "Epoch 18/50 | Train Loss: 4.1943 | Val Loss: 5.1348\n",
      "Epoch 19/50 | Train Loss: 4.0768 | Val Loss: 5.1519\n",
      "Epoch 20/50 | Train Loss: 3.9575 | Val Loss: 5.1508\n",
      "Early stopping triggered at epoch 20\n",
      "CPU times: user 1min 55s, sys: 147 ms, total: 1min 55s\n",
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 위에서 정의한 학습 루프 함수\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs, patience):\n",
    "    model.train()\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        # 훈련 시작\n",
    "        for encoder_input, decoder_input, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            encoder_input = encoder_input.to(device).long()\n",
    "            decoder_input = decoder_input.to(device).long()\n",
    "            target = target.to(device).long()\n",
    "            \n",
    "            output = model(encoder_input, decoder_input)\n",
    "            output = output.view(-1, output.shape[-1])\n",
    "            target = target.view(-1)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # 검증 시작\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for encoder_input, decoder_input, target in test_loader:\n",
    "                encoder_input = encoder_input.to(device).long()\n",
    "                decoder_input = decoder_input.to(device).long()\n",
    "                target = target.to(device).long()\n",
    "                \n",
    "                output = model(encoder_input, decoder_input)\n",
    "                output = output.view(-1, output.shape[-1])\n",
    "                target = target.view(-1)\n",
    "                loss = criterion(output, target)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # 조기 종료 조건 확인\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "            \n",
    "        model.train() # 다음 에폭을 위해 다시 훈련 모드로 전환\n",
    "\n",
    "# 학습 실행\n",
    "# 주의: 이 과정은 GPU 환경에서도 수십 분 이상 소요될 수 있습니다.\n",
    "train_model(model, train_loader, test_loader, criterion, optimizer, epochs=epochs, patience=patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60b57984-d743-4359-abf6-1c8566b07ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step4. 실제 결과와 요약문 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34827cb5-a226-4a85-9f2e-774947ccf254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추론 함수 정의 완료.\n"
     ]
    }
   ],
   "source": [
    "# --- 1.1. 정수 -> 단어 변환을 위한 딕셔너리 준비 ---\n",
    "# Step 2에서 생성한 단어 집합(vocab)을 사용합니다.\n",
    "src_index_to_word = {idx: word for word, idx in src_vocab.items()}\n",
    "tar_index_to_word = {idx: word for word, idx in tar_vocab.items()}\n",
    "\n",
    "# --- 1.2. 정수 시퀀스를 텍스트로 변환하는 헬퍼 함수 ---\n",
    "# 원문(Text) 복원\n",
    "def seq2text(input_seq):\n",
    "    # PyTorch 텐서를 순회하며 패딩(0)을 제외하고 단어로 변환합니다.\n",
    "    return ' '.join(src_index_to_word.get(i.item(), \"<UNK>\") for i in input_seq if i.item() != 0)\n",
    "\n",
    "# 실제 요약문(Summary) 복원\n",
    "def seq2summary(input_seq):\n",
    "    # 패딩(0)과 시작/종료 토큰을 제외하고 단어로 변환합니다.\n",
    "    sostoken_index = tar_word_to_index.get('sostoken', -1)\n",
    "    eostoken_index = tar_word_to_index.get('eostoken', -1)\n",
    "    return ' '.join(tar_index_to_word.get(i.item(), \"<UNK>\") for i in input_seq if i.item() not in [0, sostoken_index, eostoken_index])\n",
    "\n",
    "\n",
    "# --- 1.3. 추론(Inference) 함수 정의 ---\n",
    "# 학습된 모델로 새로운 요약문을 생성하는 핵심 함수입니다.\n",
    "def decode_sequence(input_seq, model, tar_word_to_index, tar_index_to_word, max_len_summary, device):\n",
    "    # 모델을 평가 모드로 설정\n",
    "    model.eval()\n",
    "\n",
    "    # 입력 시퀀스를 텐서로 변환하고 배치 차원을 추가\n",
    "    input_seq = input_seq.unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 1. 인코더 실행\n",
    "        encoder_outputs, hidden, cell = model.encoder(input_seq)\n",
    "\n",
    "    # 2. 디코더의 첫 입력으로 <SOS> 토큰 설정\n",
    "    decoder_input = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "    decoder_input[0, 0] = tar_word_to_index.get('sostoken', 0)\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "\n",
    "    # 3. 요약문 생성 루프\n",
    "    while not stop_condition:\n",
    "        with torch.no_grad():\n",
    "            # 이전 스텝의 hidden/cell state를 사용하여 디코더 출력 계산\n",
    "            decoder_outputs, hidden, cell = model.decoder(decoder_input, hidden, cell)\n",
    "\n",
    "            # 어텐션 적용\n",
    "            attn_out = model.attention(decoder_outputs, encoder_outputs)\n",
    "            \n",
    "            # 어텐션 결과와 디코더 출력 결합\n",
    "            concat_output = torch.tanh(model.concat(torch.cat((decoder_outputs, attn_out), dim=-1)))\n",
    "            \n",
    "            # 최종 단어 예측\n",
    "            final_output = model.output_layer(concat_output)\n",
    "\n",
    "        # 가장 확률이 높은 단어의 인덱스 선택\n",
    "        sampled_token_index = torch.argmax(final_output[0, -1, :]).item()\n",
    "        sampled_token = tar_index_to_word.get(sampled_token_index, \"<UNK>\")\n",
    "        \n",
    "        # 4. 종료 조건 확인\n",
    "        if sampled_token == 'eostoken' or len(decoded_sentence) >= (max_len_summary - 1):\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            decoded_sentence.append(sampled_token)\n",
    "\n",
    "        # 5. 다음 스텝의 입력으로 현재 예측된 단어 사용\n",
    "        decoder_input = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "        decoder_input[0, 0] = sampled_token_index\n",
    "\n",
    "    return ' '.join(decoded_sentence)\n",
    "\n",
    "print(\"추론 함수 정의 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c61185a-d516-42b9-84d7-0ffbad948aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 실제 요약 vs. 예측 요약 비교 ---\n",
      "\n",
      "--- Sample 1 ---\n",
      "원문      : order provide access information commonly searched <UNK> google rolled new feature called <UNK> search india example user searches <UNK> like <UNK> pain search app show list related conditions individual <UNK> like <UNK> show users <UNK> <UNK> along self treatment options\n",
      "실제 요약  : google rolls out <UNK> search feature in india\n",
      "예측 요약  : air india <UNK> <UNK> to <UNK> <UNK>\n",
      "\n",
      "--- Sample 2 ---\n",
      "원문      : <UNK> world cricket committee backed discussions <UNK> england wales cricket club start design project manufacturers develop head protection bowlers committee included former indian skipper sourav ganguly former australian captain <UNK> <UNK> viewed footage recent accidents involving bowlers\n",
      "실제 요약  : world cricket body backs <UNK> <UNK> for <UNK>\n",
      "예측 요약  : world cup team <UNK> <UNK> to <UNK>\n",
      "\n",
      "--- Sample 3 ---\n",
      "원문      : traders body <UNK> india traders threatened launch nationwide agitation government <UNK> billion flipkart walmart deal <UNK> claims ignoring bring reforms commerce sector encouraged walmart buy flipkart enter trade <UNK> deal increase <UNK> <UNK> commerce spectrum trader body said\n",
      "실제 요약  : <UNK> threatens nationwide <UNK> on flipkart walmart deal\n",
      "예측 요약  : india <UNK> exchange to invest billion in\n",
      "\n",
      "--- Sample 4 ---\n",
      "원문      : afghanistan cricket board cancelled scheduled series pakistan week announcing following announcement afghanistan fans started <UNK> hashtag slamming board <UNK> series pakistan calling country <UNK> terrorism <UNK> also cited recent kabul blast tweet announcing cancellation series\n",
      "실제 요약  : afghanistan cancels all <UNK> ties with pakistan\n",
      "예측 요약  : afghanistan <UNK> <UNK> <UNK> <UNK> quits\n",
      "\n",
      "--- Sample 5 ---\n",
      "원문      : two russians mistakenly reached <UNK> coast mumbai <UNK> trip tuesday according reports russian couple released police identities verified russian embassy police reportedly concerned boat managed <UNK> navy coast guard surveillance\n",
      "실제 요약  : russians reach mumbai coast by mistake\n",
      "예측 요약  : <UNK> <UNK> <UNK> arrested for <UNK> cattle\n",
      "\n",
      "--- Sample 6 ---\n",
      "원문      : bihar chief minister nitish kumar submitted resignation governor <UNK> nath tripathi wednesday reports said kumar cited crisis ruling congress rjd jd alliance reason behind resignation comes rjd lalu prasad yadav denied reports nitish kumar sought resignation lalu son deputy cm tejashwi yadav\n",
      "실제 요약  : nitish kumar resigns as bihar chief minister\n",
      "예측 요약  : kejriwal slams pm modi for <UNK> <UNK>\n",
      "\n",
      "--- Sample 7 ---\n",
      "원문      : historic meeting us president donald trump north korean leader kim jong un held june take place <UNK> hotel singapore <UNK> island area around hotel marked special security zone summit first ever sitting us president north korean leader\n",
      "실제 요약  : singapore <UNK> hotel named trump kim summit <UNK>\n",
      "예측 요약  : trump <UNK> <UNK> <UNK> drills after earthquake\n",
      "\n",
      "--- Sample 8 ---\n",
      "원문      : austrian energy drinks firm red bull created billionaires company globally according <UNK> global rich list red bull founder <UNK> <UNK> net worth billion ranked globally rich list <UNK> retailer <UNK> <UNK> thai conglomerate <UNK> <UNK> created billionaires\n",
      "실제 요약  : which company created the most billionaires globally\n",
      "예측 요약  : world largest electric bezos founder <UNK> becomes\n",
      "\n",
      "--- Sample 9 ---\n",
      "원문      : early internet <UNK> lawrence <UNK> passed away december age reportedly due heart attack <UNK> led <UNK> internet <UNK> advanced research projects agency network late leaving <UNK> helped <UNK> <UNK> <UNK> technology company <UNK>\n",
      "실제 요약  : early internet <UNK> lawrence <UNK> passes away aged\n",
      "예측 요약  : <UNK> <UNK> <UNK> <UNK> <UNK> dies at\n",
      "\n",
      "--- Sample 10 ---\n",
      "원문      : according reports makers deepika padukone shahid kapoor ranveer singh starrer padmavati planning release film executives excited watching trailer told producers <UNK> film must seen viewers enjoy filmmaker vision source said\n",
      "실제 요약  : deepika shahid starrer padmavati to release in reports\n",
      "예측 요약  : deepika to play <UNK> on rajinikanth\n",
      "\n",
      "--- Sample 11 ---\n",
      "원문      : indian benchmark index sensex fell points low morning trade tuesday amid global sell nifty fell nearly points indices seeing biggest fall since august us index <UNK> jones monday fell witnessing biggest decline since august\n",
      "실제 요약  : sensex falls over points amid global sell off\n",
      "예측 요약  : sensex falls points in <UNK> high of\n",
      "\n",
      "--- Sample 12 ---\n",
      "원문      : mumbai based social commerce startup shop raised crore series funding round led kalaari capital unilever ventures existing investors including <UNK> venture partners <UNK> capital also participated round founded iit kanpur alumni <UNK> jain aditya gupta shop lets <UNK> sell products via social media platforms nnnn\n",
      "실제 요약  : iit alumni social commerce startup shop raises crore\n",
      "예측 요약  : mumbai startup <UNK> <UNK> raises crore\n",
      "\n",
      "--- Sample 13 ---\n",
      "원문      : catalonia independence spain would enjoy international recognition france said monday ahead catalan regional government announcement last week independence vote result crisis needs resolved dialogue levels spanish politics france urged earlier catalonia claimed participants voted favour independence\n",
      "실제 요약  : catalan independence would not be <UNK> france\n",
      "예측 요약  : voter turnout recorded in meghalaya assembly elections\n",
      "\n",
      "--- Sample 14 ---\n",
      "원문      : japan public <UNK> <UNK> tuesday mistakenly sent false text alert citizens north korea launched ballistic missile towards country government <UNK> people take shelter inside buildings underground read message comes days us state hawaii issued false alert warning people incoming ballistic missile\n",
      "실제 요약  : japan mistakenly issues false korea missile attack warning\n",
      "예측 요약  : un declares un <UNK> <UNK> amid rohingya\n",
      "\n",
      "--- Sample 15 ---\n",
      "원문      : supreme court said nearly former <UNK> crorepatis per reports reacting finance minister arun jaitley said exclusive domain parliament decide <UNK> government pension much notably congress <UNK> ramesh said present mps would millionaires term ends\n",
      "실제 요약  : of former mps are crorepatis supreme court\n",
      "예측 요약  : <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> pm\n",
      "\n",
      "--- Sample 16 ---\n",
      "원문      : scientists discovered remains new species snake stomach snake <UNK> recovered mexico new species measured inches long new features including large <UNK> plates <UNK> <UNK> covering sexual organs researchers named <UNK> <UNK> meaning mysterious dinner snake\n",
      "실제 요약  : new species of snake discovered inside another snake\n",
      "예측 요약  : scientists discover <UNK> <UNK> from solar cells\n",
      "\n",
      "--- Sample 17 ---\n",
      "원문      : elon musk founder aerospace startup spacex co founder tesla launched artificial intelligence startup <UNK> wall street journal reports <UNK> create devices <UNK> human brains help keep machines february musk launched boring company startup <UNK> tunnels avoid traffic\n",
      "실제 요약  : elon musk launches artificial intelligence startup <UNK>\n",
      "예측 요약  : elon musk founder <UNK> <UNK> startup <UNK>\n",
      "\n",
      "--- Sample 18 ---\n",
      "원문      : police sunday seized huge <UNK> foreign liquor <UNK> village bihar <UNK> district basis specific intelligence <UNK> <UNK> foreign country liquor worth around crore seized various houses said police <UNK> run notably bihar became dry state april\n",
      "실제 요약  : liquor worth crore seized in dry bihar\n",
      "예측 요약  : <UNK> <UNK> <UNK> <UNK> worth crore in\n",
      "\n",
      "--- Sample 19 ---\n",
      "원문      : delhi high court asked former <UNK> promoter <UNK> singh deposit crore obtained selling shares company violating orders <UNK> used amount pay <UNK> singapore apartment avoid default order comes part case related execution crore arbitration award japanese drugmaker <UNK>\n",
      "실제 요약  : delhi hc asks <UNK> singh to deposit crore\n",
      "예측 요약  : delhi court <UNK> crore for <UNK> <UNK>\n",
      "\n",
      "--- Sample 20 ---\n",
      "원문      : us cloud computing company <UNK> overtook tesla take top spot forbes list world <UNK> companies <UNK> ranked second past two consecutive years earlier held top spot list inception notably amazon jumped eight spots come third place list\n",
      "실제 요약  : <UNK> beats tesla as world most <UNK> company\n",
      "예측 요약  : us launches its first ever <UNK> <UNK>\n"
     ]
    }
   ],
   "source": [
    "# --- 테스트 데이터셋의 일부 샘플에 대한 결과 비교 ---\n",
    "print(\"\\n--- 실제 요약 vs. 예측 요약 비교 ---\")\n",
    "\n",
    "for i in range(20): # 20개의 샘플에 대해 비교\n",
    "    input_seq = encoder_input_test[i]\n",
    "    actual_summary_seq = decoder_target_test[i] # eostoken이 포함된 버전\n",
    "    \n",
    "    # 예측 요약 생성\n",
    "    predicted_summary = decode_sequence(\n",
    "        input_seq, \n",
    "        model, \n",
    "        tar_word_to_index, \n",
    "        tar_index_to_word, \n",
    "        summary_max_len, \n",
    "        device\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n--- Sample {i+1} ---\")\n",
    "    print(\"원문      :\", seq2text(input_seq))\n",
    "    print(\"실제 요약  :\", seq2summary(actual_summary_seq))\n",
    "    print(\"예측 요약  :\", predicted_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b6a5422-3c79-4ed3-b02f-a22d64fca580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step5. Summa 를 이용한 추출적 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e8ea316-87ca-4b46-bb56-e7e9f88627e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: summa in /opt/conda/lib/python3.12/site-packages (1.2.0)\n",
      "Requirement already satisfied: scipy>=0.19 in /opt/conda/lib/python3.12/site-packages (from summa) (1.15.2)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in /opt/conda/lib/python3.12/site-packages (from scipy>=0.19->summa) (2.2.6)\n",
      "Summa 라이브러리 준비 완료.\n"
     ]
    }
   ],
   "source": [
    "!pip install summa\n",
    "import requests\n",
    "from summa.summarizer import summarize\n",
    "\n",
    "print(\"Summa 라이브러리 준비 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c9a208c-de15-4c20-82dc-ea02ae3e4a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 원문 텍스트 일부 ---\n",
      "In 1999, in an unnamed city, Computer programmer Thomas Anderson (Keanu Reeves) is secretly a hacker known as \"Neo\". He is restless, eager and driven to learn the meaning of cryptic references to the \"Matrix\" appearing on his computer. A woman named Trinity is observing Neo, and she does so knowing that Morpheus believes that Neo is \"the One\".\n",
      "\n",
      "During one of her forays, Trinity is tracked down by the local police to her hotel room. Outside the hotel a car drives up and three agents appear in neatly pressed black suits. They are Agent Smith (Hugo Weaving), Agent Brown (Paul Goddard), and Agent Jones (Robert Taylor). Trinity calls Morpheus and says that her line was tracked and Morpheus orders her to find another exit. Trinity easily defeats the six policemen sent to apprehend her, using fighting and evasion techniques that seem to defy gravity.\n",
      "\n",
      "A fierce rooftop chase ensues with Trinity and an Agent leaping impossibly from one building to the next, astonishing the policemen left be\n"
     ]
    }
   ],
   "source": [
    "# 요약할 텍스트 데이터의 URL\n",
    "url = \"https://raw.githubusercontent.com/modulabs/Aiffel_Resources/main/matrix_synopsis.txt\"\n",
    "\n",
    "# requests를 사용하여 텍스트 데이터 가져오기\n",
    "try:\n",
    "    text = requests.get(url, timeout=10).text\n",
    "    print(\"--- 원문 텍스트 일부 ---\")\n",
    "    print(text[:1000]) # 원문의 앞 1000자만 출력하여 확인\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"텍스트를 불러오는 데 실패했습니다: {e}\")\n",
    "    text = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62c126bf-54aa-49e6-8e57-c3b92de72116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 원문 길이의 5%로 추출적 요약 ---\n",
      "The Agents interrogate Neo about Morpheus, but he refuses to cooperate.\n",
      "Trinity and her allies bring Neo to Morpheus, their leader.\n",
      "\"This,\" he says, showing an image of a modern city, \"is the world that you know.\" A thing that really exists \"only as part of a neural, interactive simulation that we call the Matrix.\" Morpheus then shows Neo the world as it truly exists today, a scarred, desolate emptiness with charred, abandoned buildings, black earth, and a shrouded sky.\n",
      "Morpheus liberated Neo because he believes him to be \"the One\", a prophesied figure destined to dismantle the Matrix and liberate humanity by ending the war with the machines.\n",
      "Morpheus and Neo are walking down a standard city street in what appears to be the Matrix.\n",
      "Morpheus and Trinity use a telephone to exit the Matrix, but Neo is ambushed by Agent Smith.\n",
      "Neo revives with new power to perceive and control the Matrix, and effortlessly destroys Agent Smith, before returning to the real world in time for the ship's EMP weapon to destroy the attacking sentinels.\n"
     ]
    }
   ],
   "source": [
    "if text:\n",
    "    print(\"\\n--- 원문 길이의 5%로 추출적 요약 ---\")\n",
    "    summary_ratio = summarize(text, ratio=0.05)\n",
    "    print(summary_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2972295f-51ce-4e5f-8c65-08830d1d8f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 50단어 내외로 추출적 요약 ---\n",
      "Morpheus and Neo are walking down a standard city street in what appears to be the Matrix.\n",
      "Morpheus and Trinity use a telephone to exit the Matrix, but Neo is ambushed by Agent Smith.\n"
     ]
    }
   ],
   "source": [
    "if text:\n",
    "    print(\"\\n--- 50단어 내외로 추출적 요약 ---\")\n",
    "    summary_words = summarize(text, words=50)\n",
    "    print(summary_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1092d118-3af1-4962-a707-77815cc659e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 문장 리스트 형태로 요약 결과 받기 ---\n",
      "[1] The Agents interrogate Neo about Morpheus, but he refuses to cooperate.\n",
      "[2] Trinity and her allies bring Neo to Morpheus, their leader.\n",
      "[3] \"This,\" he says, showing an image of a modern city, \"is the world that you know.\" A thing that really exists \"only as part of a neural, interactive simulation that we call the Matrix.\" Morpheus then shows Neo the world as it truly exists today, a scarred, desolate emptiness with charred, abandoned buildings, black earth, and a shrouded sky.\n",
      "[4] Morpheus liberated Neo because he believes him to be \"the One\", a prophesied figure destined to dismantle the Matrix and liberate humanity by ending the war with the machines.\n",
      "[5] Morpheus and Neo are walking down a standard city street in what appears to be the Matrix.\n",
      "[6] Morpheus and Trinity use a telephone to exit the Matrix, but Neo is ambushed by Agent Smith.\n",
      "[7] Neo revives with new power to perceive and control the Matrix, and effortlessly destroys Agent Smith, before returning to the real world in time for the ship's EMP weapon to destroy the attacking sentinels.\n"
     ]
    }
   ],
   "source": [
    "if text:\n",
    "    print(\"\\n--- 문장 리스트 형태로 요약 결과 받기 ---\")\n",
    "    summary_sentences = summarize(text, ratio=0.05, split=True)\n",
    "    \n",
    "    for i, sentence in enumerate(summary_sentences):\n",
    "        print(f\"[{i+1}] {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8907f419-68d6-45d3-88b8-d7e7ec089d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [비교 분석] 추상적 요약 vs. 추출적 요약 ---\n",
      "\n",
      "--- Sample 1 ---\n",
      "원문      : <UNK> world cricket committee backed discussions <UNK> england wales cricket club start design project manufacturers develop head protection bowlers committee included former indian skipper sourav ganguly former australian captain <UNK> <UNK> viewed footage recent accidents involving bowlers\n",
      "실제 요약  : world cricket body backs <UNK> <UNK> for <UNK>\n",
      "추상적 요약  : world cup team <UNK> <UNK> to <UNK>\n",
      "추출적 요약  : \n",
      "\n",
      "--- Sample 5 ---\n",
      "원문      : bihar chief minister nitish kumar submitted resignation governor <UNK> nath tripathi wednesday reports said kumar cited crisis ruling congress rjd jd alliance reason behind resignation comes rjd lalu prasad yadav denied reports nitish kumar sought resignation lalu son deputy cm tejashwi yadav\n",
      "실제 요약  : nitish kumar resigns as bihar chief minister\n",
      "추상적 요약  : kejriwal slams pm modi for <UNK> <UNK>\n",
      "추출적 요약  : \n",
      "\n",
      "--- Sample 10 ---\n",
      "원문      : indian benchmark index sensex fell points low morning trade tuesday amid global sell nifty fell nearly points indices seeing biggest fall since august us index <UNK> jones monday fell witnessing biggest decline since august\n",
      "실제 요약  : sensex falls over points amid global sell off\n",
      "추상적 요약  : sensex falls points in <UNK> high of\n",
      "추출적 요약  : \n",
      "\n",
      "--- Sample 13 ---\n",
      "원문      : japan public <UNK> <UNK> tuesday mistakenly sent false text alert citizens north korea launched ballistic missile towards country government <UNK> people take shelter inside buildings underground read message comes days us state hawaii issued false alert warning people incoming ballistic missile\n",
      "실제 요약  : japan mistakenly issues false korea missile attack warning\n",
      "추상적 요약  : un declares un <UNK> <UNK> amid rohingya\n",
      "추출적 요약  : \n",
      "\n",
      "--- Sample 16 ---\n",
      "원문      : elon musk founder aerospace startup spacex co founder tesla launched artificial intelligence startup <UNK> wall street journal reports <UNK> create devices <UNK> human brains help keep machines february musk launched boring company startup <UNK> tunnels avoid traffic\n",
      "실제 요약  : elon musk launches artificial intelligence startup <UNK>\n",
      "추상적 요약  : elon musk founder <UNK> <UNK> startup <UNK>\n",
      "추출적 요약  : \n"
     ]
    }
   ],
   "source": [
    "# 비교할 테스트 데이터 샘플 인덱스 선택\n",
    "sample_indices = [1, 5, 10, 13, 16] # Step 4에서 확인한 결과 중 일부 선택\n",
    "\n",
    "print(\"--- [비교 분석] 추상적 요약 vs. 추출적 요약 ---\")\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "for index in sample_indices:\n",
    "    # 1. 비교할 원문 데이터 준비\n",
    "    original_text_sequence = encoder_input_test[index]\n",
    "    original_text = seq2text(original_text_sequence) # 정수 시퀀스를 다시 텍스트로 복원\n",
    "    actual_summary = seq2summary(decoder_target_test[index])\n",
    "\n",
    "    # 2. 추상적 요약 생성 (Step 4에서 사용한 함수 재사용)\n",
    "    abstractive_summary = decode_sequence(\n",
    "        original_text_sequence, \n",
    "        model, \n",
    "        tar_word_to_index, \n",
    "        tar_index_to_word, \n",
    "        summary_max_len, \n",
    "        device\n",
    "    )\n",
    "\n",
    "    # 3. 추출적 요약 생성 (Summa 사용)\n",
    "    # summa는 긴 텍스트에 더 적합하지만, 비교를 위해 짧은 원문에도 적용해봅니다.\n",
    "    # 문장이 하나뿐인 경우가 많아 words 기준으로 요약합니다.\n",
    "    try:\n",
    "        extractive_summary = summarize(data.iloc[index]['Text'], words=15) # 전처리 전 원본 Text 사용\n",
    "    except ValueError:\n",
    "        extractive_summary = \"요약 생성 불가 (원문이 너무 짧음)\"\n",
    "    \n",
    "    # 4. 결과 저장\n",
    "    comparison_data.append({\n",
    "        \"index\": index,\n",
    "        \"original_text\": original_text,\n",
    "        \"actual_summary\": actual_summary,\n",
    "        \"abstractive_summary\": abstractive_summary,\n",
    "        \"extractive_summary\": extractive_summary\n",
    "    })\n",
    "\n",
    "# 저장된 결과 출력\n",
    "for item in comparison_data:\n",
    "    print(f\"\\n--- Sample {item['index']} ---\")\n",
    "    print(\"원문      :\", item['original_text'])\n",
    "    print(\"실제 요약  :\", item['actual_summary'])\n",
    "    print(\"추상적 요약  :\", item['abstractive_summary'])\n",
    "    print(\"추출적 요약  :\", item['extractive_summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11ae106-94d3-49db-8850-7dbd8c9d87f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
